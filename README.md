# End-to-End ETL Pipeline using Apache Airflow, Docker & PostgreSQL

## ğŸ“– Project Overview

This project implements a production-style batch ETL pipeline that:

- Extracts raw sales transaction data from a CSV file
- Cleans and transforms the data
- Validates business rules
- Loads cleaned data into PostgreSQL
- Orchestrates the workflow using Apache Airflow
- Runs fully containerized using Docker Compose

The pipeline processes over 500,000 records.

---

## ğŸ— Architecture

Raw CSV (sales.csv)
        â†“
   Extract Task
        â†“
   Transform Task
        â†“
   Validate Task
        â†“
   Load Task
        â†“
   PostgreSQL Database

Orchestration Layer: Apache Airflow  
Containerization: Docker Compose  

---

## ğŸ›  Tech Stack

- Python
- Pandas
- Apache Airflow
- Docker & Docker Compose
- PostgreSQL
- SQLAlchemy
- psycopg2

---

## ğŸ”„ Pipeline Steps

### 1ï¸âƒ£ Extract
- Reads raw CSV data
- Logs raw row count

### 2ï¸âƒ£ Transform
- Removes duplicates
- Removes null values
- Converts Date column to datetime
- Standardizes column names
- Removes negative or zero quantity
- Removes negative or zero price
- Saves cleaned dataset

### 3ï¸âƒ£ Validate
- Checks required columns
- Checks for null values
- Enforces business rules
- Fails pipeline if validation fails

### 4ï¸âƒ£ Load
- Connects to PostgreSQL
- Inserts validated data into `sales` table

---

## ğŸ“Š Data Volume

- ~536,000 raw records  
- ~531,000 cleaned records  

---

## ğŸš€ How to Run

1. Initialize Airflow:

docker compose up airflow-init

2. Start all services:

docker compose up

3. Open Airflow UI:

http://localhost:8080

---

## ğŸ“Œ Key Engineering Concepts Demonstrated

- Batch ETL design
- DAG orchestration
- Data validation gate
- Containerized services
- PostgreSQL integration
- Error handling via Airflow
- Logging for observability
